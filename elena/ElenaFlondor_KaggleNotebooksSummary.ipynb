{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains some short reviews of the most popular notebooks from the Kaggle competition: https://www.kaggle.com/mlg-ulb/creditcardfraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color='red'>Each reviewed notebook will have the following structure:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook link:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short description of the goals of the notebook: for example dataset exploration, DNN approach to predict the fraudulent transaction, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method1: name; pros; cons;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Credit Fraud || Dealing with Imbalanced Datasets\n",
    "<b>link: https://www.kaggle.com/janiobachmann/credit-fraud-dealing-with-imbalanced-datasets </b>\n",
    "\n",
    "###### ! notebook to detect patterns that give signs of fraud\n",
    "\n",
    "Notebook content:\n",
    "- dataset distribution analyze\n",
    "- NearMiss algorithm\n",
    "- various predictive models compared \n",
    "- mistakes due to imbalance datasets\n",
    "\n",
    "OBS: \n",
    "- imbalanced dataset\n",
    "    - <b>!!!SMOTE</b> used to create new synthetic points to obtain balanced classes. \n",
    "    \n",
    "- sub-sample to avoid overfitting and wrong corrleations\n",
    "- split dataset in train and test sets\n",
    "- equally distributed classes based on Random Under Sampling \n",
    "- anomaly detection - to remove extreme outliers from features that have high correlation with our classes\n",
    "- clustering and dimensionality reduction based on t-SNE\n",
    "\n",
    "\n",
    "#### Classification methods\n",
    "##### Method 1: Logistic Regression\n",
    "- the best (95% acc)\n",
    "\n",
    "##### Method 2: KNeighborsClassifier\n",
    "- 93% acc\n",
    "\n",
    "##### Method 3: SVC\n",
    "- 92% acc\n",
    "\n",
    "##### Method 4: DecisionTreeClassifier\n",
    "- the worst (88% acc)\n",
    "\n",
    "(the order is kept in the case of cross validation, too)\n",
    "\n",
    "- LOGISTIC REGRESSION - DEEPER ANALYZED\n",
    "- NEURAL NETWORK \n",
    "    - simple one: inpul layer, 1 hidden layer (32 neurons), output layer (binary classification)\n",
    "    - AdamOptimizer used with the defaulr learning rate \n",
    "\n",
    "\n",
    "Suggestion: Split in train, validation, test\n",
    "\n",
    "## 2.  In depth skewed data classif. (93% recall acc now)\n",
    "<b>link: https://www.kaggle.com/joparga3/in-depth-skewed-data-classif-93-recall-acc-now</b>\n",
    "\n",
    "###### ! notebook to test different methods on skewed data\n",
    "\n",
    "Notebook content:\n",
    "- presents some ways to approach the classification problem considering the umbalance\n",
    "- some performane metrics:\n",
    "    - ROC curves\n",
    "    - Precision and Recall from confusion matrix\n",
    "    - F1score \n",
    "    - Kappa score \n",
    "- resampling \n",
    "    - method that processes the data to have an approximate 50-50 ratio\n",
    "    - OVER-samplig \n",
    "    - UNDER-sampling\n",
    "- split the dataset in train and test \n",
    "- methods: \n",
    "    - LogisticRegressionClassifier with undersampled data \n",
    "    - LogisticRegressionClassifier on skewed data\n",
    "- cross validation used for accuracy\n",
    "\n",
    "Result: 93.2% accuracy on the undersampled dataset for test set\n",
    "\n",
    "## 3. Predicting Credit Card Fraud¶\n",
    "<b>link: https://www.kaggle.com/currie32/predicting-fraud-with-tensorflow</b>\n",
    "\n",
    "###### ! the goal is to predict credit card fraud + visualize the dataset\n",
    "\n",
    "Notebook content:\n",
    "- dataset exploration - check if missing data, time comparisons across fraudulent and normal transactions\n",
    "- NN \n",
    "    - Vriable layers used - what is this?!?! \n",
    "    - seems to be basic, sigmoid activation + dropout, last is softmax\n",
    "- t-SNE - data visualization\n",
    "    - applied on the original data and with the data used to train the NN\n",
    "    - interesting plot - identifies two main groups of fraudulent transactions \n",
    "    \n",
    "    \n",
    "## 4. Automated Hyperparameter Tuning¶\n",
    "<b>link: https://www.kaggle.com/pavansanagapati/automated-hyperparameter-tuning</b>\n",
    "\n",
    "###### ! hyperparameter tunning study - tools\n",
    "\n",
    "Notebook content:\n",
    "- hyperparameter tunning - concept presentation\n",
    "- random forests (manual search + random search + grid search)\n",
    "- how to perform automated hyperparameter tuning and choose the best parameters indentified \n",
    "- genetic optimization (TPOTClassifier)\n",
    "- optuna (svm and random forest classifier) - offers easy parallelization\n",
    "- tune \n",
    "- sherpa\n",
    "\n",
    "! needs a lot more research\n",
    "\n",
    "\n",
    "## 5. Credit Card Fraud Detection Predictive Models¶\n",
    "<b>link: https://www.kaggle.com/gpreda/credit-card-fraud-detection-predictive-models</b>\n",
    "\n",
    "###### basic study - almost the same structure as the above notebooks\n",
    "\n",
    "Notebook content:\n",
    "- dataset processing\n",
    "- dataset exploration\n",
    "- tried a few predictive models:\n",
    "    - RandomForrestClassifier\n",
    "    - AdaBoostClassifier\n",
    "    - CatBoostClassifier\n",
    "    - XGBoost\n",
    "    - LightGBM\n",
    "\n",
    "! nothing spectacular, but nice structured\n",
    "\n",
    "## 6. Fraud Detection Balancing, ROC & PR curves\n",
    "<b>link: https://www.kaggle.com/chirag19/fraud-detection-balancing-roc-pr-curves</b>\n",
    "\n",
    "###### compare various data balancing techniques with powerful boosting models\n",
    "\n",
    "Notebook content:\n",
    "- imbalanced and balanced data set problem is studied\n",
    "    - down-sampling \n",
    "    - up-sampling\n",
    "    - SMOTE - regular and BorderLine\n",
    "    - ADASYN = SMOTE + random values between 0 and 1. This makes dataset somewhat more robust.\n",
    "- compares Random Forest, XGBoost and LGB based on the ROC curve\n",
    "\n",
    "## 7. Outlier!!! The Silent Killer\n",
    "<b>link: https://www.kaggle.com/nareshbhat/outlier-the-silent-killer</b>\n",
    "\n",
    "###### outlier study \n",
    "\n",
    "Notebook content: \n",
    "- different outlier detection technique\n",
    "    1. Hypothesis Testing\n",
    "    2. Z-score method\n",
    "    3. Robust Z-score\n",
    "    4. I.Q.R method\n",
    "    5. Winsorization method(Percentile Capping)\n",
    "    6. DBSCAN Clustering\n",
    "    7. Isolation Forest\n",
    "    8. Visualizing the data\n",
    "\n",
    "\n",
    "## 8. Credit Card Fraud Detection (f1-score=0.86)\n",
    "\n",
    "<b>link: https://www.kaggle.com/mariapushkareva/credit-card-fraud-detection-f1-score-0-86</b>\n",
    "\n",
    "\n",
    "Notebook content: \n",
    "- EDA\n",
    "- methods:\n",
    "    - Logistic Regression without SMOTE\n",
    "    - Logistic Regression with SMOTE\n",
    "    - Random Forest Classifier with SMOTE\n",
    "    - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
